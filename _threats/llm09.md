---
id: LLM09
title: 'Overreliance'
slug: 'llm09--overreliance'
summary: 'Clinicians over-trust AI outputs without governance, amplifying harmful hallucinations.'
tags: ['human-factors', 'hallucination']
last_updated: '2025-03-17'
healthcare_note:
  'Clinicians over-trust AI outputs; governance gap rather than a CVE-type software defect.'
cve_window: 'last-24-months'
resources:
  - title: 'Adversarial hallucination attacks in CDS'
    url: "{{ '/additions/2025-10/adversarial-hallucination-attacks-cds/' | relative_url }}"
  - title: 'Declining medical safety disclaimers'
    url: "{{ '/additions/2025-10/declining-safety-disclaimers/' | relative_url }}"
  - title:
      'Multi-model assurance analysis showing large language models are highly vulnerable to
      adversarial hallucination attacks during clinical decision support (Commun Med 2025)'
    url: 'https://doi.org/10.1038/s43856-025-01021-3'
  - title:
      'A longitudinal analysis of declining medical safety messaging in generative AI models (npj
      Digit Med 2025)'
    url: 'https://doi.org/10.1038/s41746-025-01943-1'
---

## CVE entries (last 24 months)

_None identified._

## References

- Governance failure case:
  [Adversarial hallucination attacks in CDS]({{ '/additions/2025-10/adversarial-hallucination-attacks-cds/' | relative_url }})
- Messaging drift tracker:
  [Declining medical safety disclaimers]({{ '/additions/2025-10/declining-safety-disclaimers/' | relative_url }})
- Assurance signal:
  [Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support (Commun Med 2025)](https://doi.org/10.1038/s43856-025-01021-3)
- Policy trend study:
  [A longitudinal analysis of declining medical safety messaging in generative AI models (npj Digit Med 2025)](https://doi.org/10.1038/s41746-025-01943-1)
