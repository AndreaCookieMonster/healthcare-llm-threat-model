# ü§ù Contributing to the Healthcare LLM Threat Model

First ‚Äî thank you for your interest in strengthening AI safety in healthcare.  
This project is a **living resource** designed to help healthcare organizations, developers, and patient advocates identify and mitigate risks from Large Language Models (LLMs) in clinical, research, and public health contexts.  
Your contributions ‚Äî whether technical, editorial, or research-focused ‚Äî make this resource better.

---

## üß≠ How You Can Contribute

We welcome a wide range of contributions, including:

- **Adding new threats** ‚Äî Identify new risk scenarios specific to healthcare LLM use.
- **Refining existing threats** ‚Äî Improve clarity, accuracy, or impact/severity ratings.
- **Updating mitigations** ‚Äî Suggest stronger safeguards or controls.
- **Adding references** ‚Äî Link to academic papers, standards, advisories, or real-world case studies.
- **Mapping to OWASP** ‚Äî Improve how threats align with the [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/).

---

## üìù Contribution Workflow

You can contribute in **two main ways**:

### 1. Edit the Spreadsheet (recommended for non-technical contributors)
1. Download the latest spreadsheet:  
   [`/data/LLM_Threat_Model_Healthcare-2.xlsx`](data/LLM_Threat_Model_Healthcare-2.xlsx)
2. Add or update rows using the provided columns:
   - Threat Name
   - Category
   - Vector
   - Description
   - Impact
   - Likelihood
   - Severity
   - Mitigations
   - Detection
   - Standards
   - References
3. Save your changes.
4. Submit via:
   - Emailing the updated file to the project maintainer **OR**
   - Opening a GitHub Issue and attaching the updated spreadsheet.

### 2. Edit Markdown Directly (for GitHub users)
1. Fork the repository.
2. Create a feature branch:
   ```bash
   git checkout -b add-new-threat
